{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXTQP7XwTaKMYOIMgg8NU+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 03 - Feature Engineering"],"metadata":{"id":"phGaDxVuzmAv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vC_kYwkYU3sr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729477747117,"user_tz":300,"elapsed":19963,"user":{"displayName":"Matt Flaherty","userId":"08330532474898317602"}},"outputId":"6a33f570-130f-4a59-89be-4155b1ddd1fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/BDB 2025\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","cur_path = \"/content/drive/MyDrive/BDB 2025/\"\n","os.chdir(cur_path)\n","!pwd"]},{"cell_type":"code","source":["!pip install pyspark\n","\n","# The entry point to programming Spark with the DataFrame API.\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.master(\"local[*]\").appName(\"DataFrame\").getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vQGrsgdzK16","executionInfo":{"status":"ok","timestamp":1729477764506,"user_tz":300,"elapsed":17392,"user":{"displayName":"Matt Flaherty","userId":"08330532474898317602"}},"outputId":"5dfba351-6fc3-4c9e-e5a6-04ea19eb1dd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","pd.set_option('display.max_columns', 500)\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import sqrt\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.feature import OneHotEncoder\n","from pyspark.sql.types import IntegerType\n","\n","import time"],"metadata":{"id":"VLOmKcOxzM1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["games = spark.read.option(\"header\",'True').csv('./data/games.csv')\n","players = spark.read.option(\"header\",'True').csv('./data/players.csv')\n","plays = spark.read.option(\"header\",'True').csv('./data/plays.csv')\n","player_play = spark.read.option(\"header\",'True').csv('./data/player_play.csv')\n","tracking = spark.read.option(\"header\",'True').parquet('./data/tracking_week*.parquet')"],"metadata":{"id":"YyQJbuCwzPJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## One hot encoding\n","\n","One hot encode the teams to add context to the model. Not every team will run each route at the same rate."],"metadata":{"id":"of0SbLfXz6x_"}},{"cell_type":"code","source":["# filter to offense, don't care about defensive tendencies\n","onehot = tracking.filter(tracking.isOnOffense==1)\n","\n","# convert club to integer for one hot encoding\n","indexer = StringIndexer(inputCol=\"club\", outputCol=\"clubId\")\n","indexed = indexer.fit(onehot).transform(onehot)\n","\n","# one hot encode club\n","encoder = OneHotEncoder(inputCols=[\"clubId\"],\n","                        outputCols=[\"offenseVec\"])\n","model = encoder.fit(indexed)\n","encoded = model.transform(indexed)\n","# encoded.show(5)"],"metadata":{"id":"_lIznNlrzYgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pivot Original Table\n","\n","Pivot the original tracking table to get all players on one row. Additionally, get the distances to every player on the field."],"metadata":{"id":"x_PTeYoY0vGr"}},{"cell_type":"code","source":["# join table to itself to get distance to each player on the field\n","df = tracking.join(tracking.select('gameId','playId','frameId',col(\"nflId\").alias(\"nflId2\"),col(\"isOnOffense\").alias(\"isOnOffense2\"),\n","                                   col(\"club\").alias(\"club2\"),col(\"X_std\").alias(\"X_std2\"),col(\"Y_std\").alias(\"Y_std2\"),col(\"dir_std\").alias(\"dir_std2\"),\n","                                   col(\"o_std\").alias(\"o_std2\"),col(\"s\").alias(\"s2\"),col(\"a\").alias(\"a2\"),col(\"dis\").alias(\"dis2\")),\n","                   on=['gameId','playId','frameId'],how='left')#.show(5)\n","\n","# filter to make it easier to view for now\n","# df = df.filter((df.gameId=='2022100901') & (df.playId=='1536') & (df.frameId=='60'))\n","\n","# calculate distance\n","df = df.withColumn('dist',sqrt(pow(df.X_std-df.X_std2,2)+pow(df.Y_std-df.Y_std2,2)))\n","# remove where joined player equals main player\n","df = df.filter((df.nflId!=df.nflId2))\n","# each player should have 11 players on defense and 11 players on offense\n","window_spec = Window.partitionBy('gameId','playId','frameId','nflId','isOnOffense2','club2').orderBy(\"dist\")\n","df = df.withColumn(\"row_number\",row_number().over(window_spec))\n","# add offense/defense to the row number\n","df = df.withColumn(\"offDef\", F.when((F.col('isOnOffense2') == '0') & (F.col('club2') != 'football'),concat(col('row_number'),lit('_def'))).otherwise(concat(col('row_number'),lit('_off'))))\n","# special value for the football\n","df = df.withColumn(\"offDef\", F.when((F.col('club2') == 'football'),'football').otherwise(df.offDef))\n","# df = df.filter(df.nflId=='37075')\n","# df.show()"],"metadata":{"id":"YGxFRtfU_BXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","\n","games = games.filter((games.week=='2') | (games.week=='4')).sort(\"week\", \"gameId\", ascending=[True, True])\n","for week in games.select('week').distinct().collect():\n","  print(f'Week {week[0]} start')\n","  # Create an empty RDD\n","  emp_RDD = spark.sparkContext.emptyRDD()\n","  # Create empty schema\n","  columns = StructType([])\n","  # Create an empty RDD with empty schema\n","  final_df = spark.createDataFrame(data = emp_RDD,schema = columns)\n","  for game in games.filter(games.week==week[0]).select('gameId').collect():\n","    # pivot table to get each frame on the same row for each player\n","    tracking_dist = df.filter((df.gameId == game[0])).groupBy(['gameId','playId','frameId','nflId']).\\\n","                        pivot('offDef').\\\n","                        agg(F.first('dist'),F.first('dir_std2'),F.first('o_std2'),F.first('s2'),F.first('a2'),F.first('dis2'))\n","    # only look at offensive players since they are the ones running the routes\n","    # join back to main df\n","    offensive_tracking = tracking.filter(tracking.isOnOffense==1).join(tracking_dist,on=['gameId','playId','frameId','nflId'])\n","    # only look at the players that ran routes\n","    offensive_tracking = offensive_tracking.join(player_play.filter(player_play.wasRunningRoute == '1').\\\n","                                                select('gameId','playId','nflId','routeRan'),on=['gameId','playId','nflId'],how='inner')\n","    if final_df.count()==0:\n","      final_df = offensive_tracking\n","      print('Initialized data')\n","    else:\n","      final_df = final_df.union(offensive_tracking)\n","      print('Unioned data')\n","    # track games\n","    print(game[0])\n","  # offensive_tracking.show(5)\n","  final_df.write.option(\"header\",True).mode('overwrite').parquet(f\"./data/games/tracking_week_{week[0]}.parquet\")\n","  final_df = final_df.unpersist()\n","  print(f'Week {week[0]} complete')\n","  end1 = time.time()\n","  print(f'total time elapsed for week {week[0]}: {end1-start}')\n","\n","end = time.time()\n","print(f'total time elapsed: {end-start}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8iP1Wec9Xul","executionInfo":{"status":"ok","timestamp":1729489286896,"user_tz":300,"elapsed":11491558,"user":{"displayName":"Matt Flaherty","userId":"08330532474898317602"}},"outputId":"b6a050b5-a330-4ca6-91f5-ccbcb5c73cbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Week 4 start\n","Initialized data\n","2022092900\n","Unioned data\n","2022100200\n","Unioned data\n","2022100201\n","Unioned data\n","2022100202\n","Unioned data\n","2022100203\n","Unioned data\n","2022100204\n","Unioned data\n","2022100205\n","Unioned data\n","2022100206\n","Unioned data\n","2022100207\n","Unioned data\n","2022100208\n","Unioned data\n","2022100209\n","Unioned data\n","2022100210\n","Unioned data\n","2022100211\n","Unioned data\n","2022100212\n","Unioned data\n","2022100213\n","Unioned data\n","2022100300\n","Week 4 complete\n","total time elapsed for week 4: 5601.552707195282\n","Week 2 start\n","Initialized data\n","2022091500\n","Unioned data\n","2022091800\n","Unioned data\n","2022091801\n","Unioned data\n","2022091802\n","Unioned data\n","2022091803\n","Unioned data\n","2022091804\n","Unioned data\n","2022091805\n","Unioned data\n","2022091806\n","Unioned data\n","2022091807\n","Unioned data\n","2022091808\n","Unioned data\n","2022091809\n","Unioned data\n","2022091810\n","Unioned data\n","2022091811\n","Unioned data\n","2022091812\n","Unioned data\n","2022091900\n","Unioned data\n","2022091901\n","Week 2 complete\n","total time elapsed for week 2: 11491.418672800064\n","total time elapsed: 11491.420011281967\n"]}]},{"cell_type":"code","source":["# add time remaining to the plays table using pandas\n","# time remaining\n","# include seconds remaining in the half because plays at the end of the quarter vs beginning of the quarter in the same half will behave the same\n","# whereas plays at the end of a half and beginning of the next half will behave differently\n","splitTime = plays['gameClock'].str.split(':', n=1, expand=True).rename(columns={0:'minutes',1:'seconds'})\n","plays['secRemainHalf'] = splitTime['minutes'].astype(int) * 60 + splitTime['seconds'].astype(int)\n","plays['secRemainHalf'] = np.where(plays['quarter'].isin([1,3]),plays['secRemainHalf']+900,plays['secRemainHalf'])"],"metadata":{"id":"sWqO0eT8nJEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plays.to_csv('./data/plays.csv',index=False)"],"metadata":{"id":"KJSNW3lq0hor"},"execution_count":null,"outputs":[]}]}